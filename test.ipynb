{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
    "    key = rngs.params()\n",
    "    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n",
    "    self.b = nnx.Param(jnp.zeros((dout,)))\n",
    "    self.din, self.dout = din, dout\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is y [[1.245453   0.74195766 0.8553282  0.6763327  1.2617068 ]]\n",
      "Linear(\n",
      "  w=Param(\n",
      "    value=Array(shape=(2, 5), dtype=float32)\n",
      "  ),\n",
      "  b=Param(\n",
      "    value=Array(shape=(5,), dtype=float32)\n",
      "  ),\n",
      "  din=2,\n",
      "  dout=5\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Linear(2, 5, rngs=nnx.Rngs(params=0))\n",
    "y = model(x=jnp.ones((1, 2)))\n",
    "\n",
    "print(f\"This is y {y}\")\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the type of count inside __init__ function <class '__main__.Count'>\n",
      "counter.count.value = Array(0, dtype=int32, weak_type=True)\n",
      "This is type of self.count before addition operation <class '__main__.Count'>\n",
      "This is type of self.count before <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "This is type of self.count after conversion <class '__main__.Count'>\n",
      "counter.count.value = Array(1, dtype=int32, weak_type=True)\n"
     ]
    }
   ],
   "source": [
    "class Count(nnx.Variable): pass\n",
    "\n",
    "class Counter(nnx.Module):\n",
    "  def __init__(self):\n",
    "    self.count = Count(jnp.array(0))\n",
    "    print(f'This is the type of count inside __init__ function {type(self.count)}')\n",
    "\n",
    "  def __call__(self):\n",
    "    print(f'This is type of self.count before addition operation {type(self.count)}')\n",
    "    self.count = self.count+1\n",
    "    print(f'This is type of self.count before {type(self.count)}')\n",
    "    self.count = Count(self.count)\n",
    "    print(f'This is type of self.count after conversion {type(self.count)}')\n",
    "\n",
    "counter = Counter()\n",
    "print(f'{counter.count.value = }')\n",
    "counter()\n",
    "print(f'{counter.count.value = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n",
    "    self.linear1 = Linear(din, dmid, rngs=rngs)\n",
    "    self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n",
    "    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n",
    "    self.linear2 = Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    x = nnx.gelu(self.dropout(self.bn(self.linear1(x))))\n",
    "    return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(2, 16, 5, rngs=nnx.Rngs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x=jnp.ones((3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  linear1=Linear(\n",
      "    w=Param(\n",
      "      value=Array(shape=(2, 16), dtype=float32)\n",
      "    ),\n",
      "    b=Param(\n",
      "      value=Array(shape=(16,), dtype=float32)\n",
      "    ),\n",
      "    din=2,\n",
      "    dout=16\n",
      "  ),\n",
      "  dropout=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
      "    default=RngStream(\n",
      "      key=RngKey(\n",
      "        value=Array((), dtype=key<fry>) overlaying:\n",
      "        [0 0],\n",
      "        tag='default'\n",
      "      ),\n",
      "      count=RngCount(\n",
      "        value=Array(5, dtype=uint32),\n",
      "        tag='default'\n",
      "      )\n",
      "    )\n",
      "  )),\n",
      "  bn=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(16,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(16,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(16,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(16,), dtype=float32)\n",
      "    ),\n",
      "    num_features=16,\n",
      "    use_running_average=False,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x000001FB2D3CE3E0>,\n",
      "    scale_init=<function ones at 0x000001FB2D47BD80>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear2=Linear(\n",
      "    w=Param(\n",
      "      value=Array(shape=(16, 5), dtype=float32)\n",
      "    ),\n",
      "    b=Param(\n",
      "      value=Array(shape=(5,), dtype=float32)\n",
      "    ),\n",
      "    din=16,\n",
      "    dout=5\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  linear1=Linear(\n",
      "    w=Param(\n",
      "      value=Array(shape=(2, 32), dtype=float32)\n",
      "    ),\n",
      "    b=Param(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    din=2,\n",
      "    dout=32\n",
      "  ),\n",
      "  dropout=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
      "    default=RngStream(\n",
      "      key=RngKey(\n",
      "        value=Array((), dtype=key<fry>) overlaying:\n",
      "        [0 0],\n",
      "        tag='default'\n",
      "      ),\n",
      "      count=RngCount(\n",
      "        value=Array(4, dtype=uint32),\n",
      "        tag='default'\n",
      "      )\n",
      "    )\n",
      "  )),\n",
      "  bn=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    num_features=32,\n",
      "    use_running_average=False,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x000001FB2D3CE3E0>,\n",
      "    scale_init=<function ones at 0x000001FB2D47BD80>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear2=Linear(\n",
      "    w=Param(\n",
      "      value=Array(shape=(32, 5), dtype=float32)\n",
      "    ),\n",
      "    b=Param(\n",
      "      value=Array(shape=(5,), dtype=float32)\n",
      "    ),\n",
      "    din=32,\n",
      "    dout=5\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LoraParam(nnx.Param): pass\n",
    "\n",
    "class LoraLinear(nnx.Module):\n",
    "  def __init__(self, linear: Linear, rank: int, rngs: nnx.Rngs):\n",
    "    self.linear = linear\n",
    "    self.A = LoraParam(jax.random.normal(rngs(), (linear.din, rank)))\n",
    "    self.B = LoraParam(jax.random.normal(rngs(), (rank, linear.dout)))\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    return self.linear(x) + x @ self.A @ self.B\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "model = MLP(2, 32, 5, rngs=rngs)\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear1 = LoraLinear(model.linear1, 4, rngs=rngs)\n",
    "model.linear2 = LoraLinear(model.linear2, 4, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  linear1=LoraLinear(\n",
      "    linear=Linear(\n",
      "      w=Param(\n",
      "        value=Array(shape=(2, 32), dtype=float32)\n",
      "      ),\n",
      "      b=Param(\n",
      "        value=Array(shape=(32,), dtype=float32)\n",
      "      ),\n",
      "      din=2,\n",
      "      dout=32\n",
      "    ),\n",
      "    A=LoraParam(\n",
      "      value=Array(shape=(2, 4), dtype=float32)\n",
      "    ),\n",
      "    B=LoraParam(\n",
      "      value=Array(shape=(4, 32), dtype=float32)\n",
      "    )\n",
      "  ),\n",
      "  dropout=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
      "    default=RngStream(\n",
      "      key=RngKey(\n",
      "        value=Array((), dtype=key<fry>) overlaying:\n",
      "        [0 0],\n",
      "        tag='default'\n",
      "      ),\n",
      "      count=RngCount(\n",
      "        value=Array(9, dtype=uint32),\n",
      "        tag='default'\n",
      "      )\n",
      "    )\n",
      "  )),\n",
      "  bn=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(32,), dtype=float32)\n",
      "    ),\n",
      "    num_features=32,\n",
      "    use_running_average=False,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x000001FB2D3CE3E0>,\n",
      "    scale_init=<function ones at 0x000001FB2D47BD80>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear2=LoraLinear(\n",
      "    linear=Linear(\n",
      "      w=Param(\n",
      "        value=Array(shape=(32, 5), dtype=float32)\n",
      "      ),\n",
      "      b=Param(\n",
      "        value=Array(shape=(5,), dtype=float32)\n",
      "      ),\n",
      "      din=32,\n",
      "      dout=5\n",
      "    ),\n",
      "    A=LoraParam(\n",
      "      value=Array(shape=(32, 4), dtype=float32)\n",
      "    ),\n",
      "    B=LoraParam(\n",
      "      value=Array(shape=(4, 5), dtype=float32)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "y = model(x=jnp.ones((3, 2)))\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An MLP containing 2 custom `Linear` layers, 1 `nnx.Dropout` layer, 1 `nnx.BatchNorm` layer.\n",
    "model = MLP(2, 16, 10, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit  # Automatic state management\n",
    "def train_step(model, optimizer, x, y):\n",
    "  def loss_fn(model: MLP):\n",
    "    y_pred = model(x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(grads)  # In place updates.\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = jnp.ones((5, 2)), jnp.ones((5, 10))\n",
    "loss = train_step(model, optimizer, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = Array(1.0000278, dtype=float32)\n",
      "optimizer.step.value = Array(1, dtype=uint32)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f'{loss = }')\n",
    "print(f'{optimizer.step.value = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
